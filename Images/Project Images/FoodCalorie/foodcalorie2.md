        - **:orange[Bowl Detection and Cropping]** Once the bowl is detected, the bounding box coordinates are used
        to crop the detected bowl from the image. This step ensures that only the relevant portion of the image,
        which contains the food item, is considered for subsequent analysis. This cropped image serves as the
        input for the next stage of the pipeline. The precise detection and cropping of the bowl are crucial as
        they directly impact the accuracy of the food classification and subsequent calculations.

- **:orange[Food Classification and Area Calculation]** The cropped image of the bowl is then passed through
        a fine-tuned YOLO V8 model, referred to as "best.pt," which is designed to classify the food item within
        the bowl. This model has been trained on a dataset containing 15 different food classes, ensuring robust
        classification capabilities. Once the food item is classified, the system calculates the area of the food
        based on the bounding box generated by the classification model. The area calculation is essential for
        estimating the weight of the food item accurately.
- **:orange[Weight and Calorie Estimation]**  With the calculated area, the system employs a pre-trained KNN
        model to predict the weight of the food item. The KNN model uses the area as an input feature to
        provide an accurate weight estimation. Subsequently, the calorie content of the food is calculated using
        a dictionary of food items and their respective calorie values per 100 grams.The system aggregates the unique food items, their counts, total calorie content, and total weight, and outputs these values. This comprehensive output provides valuable information for dietary monitoring and nutritional assessments.


